<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
  <title>Setting up the Board and Application Deployment &mdash; Kria™ KV260 2020.2 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Customizing the AI Models used in the application" href="customize_ai_models_nlp.html" />
    <link rel="prev" title="Design Overview" href="introduction_nlp.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../../index.html" class="icon icon-home"> Kria™ KV260
            <img src="../../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2020.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/index.html">SOM</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">KV260 Applications</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../smartcamera/smartcamera_landing.html">Smart Camera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aibox-reid/aibox_landing.html">AIBox-ReID</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../defect-detect/defectdetect_landing.html">Defect Detect</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../nlp_smartvision_landing.html">NLP SmartVision</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#overview">Overview</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../nlp_smartvision_landing.html#quick-start">Quick Start</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Setting up the Board and Application deployment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-the-board">Setting up the Board</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-the-application">Run the Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-based-testing-and-accuracy-measurement-of-kws-only">File based Testing and Accuracy Measurement of KWS Only</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-based-testing-of-dpu-only">Image based Testing of DPU Only</a></li>
<li class="toctree-l4"><a class="reference internal" href="#files-structure-of-the-application">Files structure of the application</a></li>
<li class="toctree-l4"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#tutorials">Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#architecture">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#other">Other</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Releases</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/main/build/html/index.html">Kria KV260</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Releases</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pages.gitenterprise.xilinx.com/SOM/kv260-docs/2020.2/build/html/index.html">2021.2</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Kria™ KV260</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../nlp_smartvision_landing.html">NLP SmartVision</a> &raquo;</li>
      <li>Setting up the Board and Application Deployment</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/nlp-smartvision/docs/app_deployment_nlp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <table class="sphinxhide">
 <tr>
   <td align="center"><img src="../../media/xilinx-logo.png" width="30%"/><h1> Kria&trade; KV260 Vision AI Starter Kit NLP SmartVision Tutorial</h1>
   </td>
 </tr>
 <tr>
 <td align="center"><h1>Setting up the Board and Application Deployment</h1> </td>
 </tr>
</table><div class="section" id="setting-up-the-board-and-application-deployment">
<h1>Setting up the Board and Application Deployment<a class="headerlink" href="#setting-up-the-board-and-application-deployment" title="Permalink to this heading">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>This document shows how to set up the board and run the nlp-smartvision application.</p>
</div>
<div class="section" id="setting-up-the-board">
<h2>Setting up the Board<a class="headerlink" href="#setting-up-the-board" title="Permalink to this heading">¶</a></h2>
<ol>
<li><p>Flash the SD Card</p>
<ul class="simple">
<li><p>Download  the <a class="reference external" href="https://www.xilinx.com/member/forms/download/xef.html?filename=petalinux-sdimage_0415.wic.gz">SD Card Image</a> and save it on your computer.</p></li>
<li><p>Connect the microSD to your computer.</p></li>
<li><p>Download the <a class="reference external" href="https://www.balena.io/etcher/">Balena Etcher tool</a> (recommended; available for Window, Linux, and
macOS) required to flash the SD card.</p></li>
<li><p>Follow the instructions in the tool and select the downloaded image to flash onto your microSD card.</p></li>
</ul>
<p><img alt="Balena Etcher" src="../../../_images/balena.png" /></p>
<ul class="simple">
<li><p>Eject the SD card from your computer.</p></li>
</ul>
<p>If you are looking for other OS specific tools to write the image to the SD card refer to <a class="reference external" href="https://www.xilinx.com/products/som/kria/kv260-vision-starter-kit/kv260-getting-started/setting-up-the-sd-card-image.html">KV260 Getting Started Page</a></p>
</li>
<li><p>Hardware Setup:</p>
<p><img alt="GitHub Logo" src="../../../_images/som-board.png" /></p>
<ul>
<li><p>Monitor:</p>
<p>Before booting, connect the monitor which supports 1024x768 resolution to the board via DP/HDMI port.</p>
</li>
<li><p>IAS sensor:</p>
<p>Before power on, install an AR1335 sensor module in J7. Make sure there is no other camera interface connected to the setup.</p>
</li>
<li><p>UART/JTAG interface:</p>
<p>For interacting and seeing boot-time information, connect a USB debugger to the J4.</p>
</li>
<li><p>USB Microphone:</p>
<p>Connect the microphone to any of the USB ports.</p>
</li>
<li><p>Network connection:</p>
<p>Connect the Ethernet cable to your local network with DHCP enabled.</p>
</li>
</ul>
</li>
<li><p>Power on the board, and boot the Linux image.</p>
<p>The Linux image will boot into the following login prompt:</p>
<p><code class="docutils literal notranslate"><span class="pre">xilinx-k26-starterkit-2020_2</span> <span class="pre">login:</span></code></p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">petalinux</span></code> user for login. You will be prompted to set a new password
on the first login.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xilinx-k26-starterkit-2020_2 login: petalinux
You are required to change your password immediately <span class="o">(</span>administrator enforced<span class="o">)</span>
New password:
Retype new password:
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">petalinux</span></code> user does not have root privileges. Most commands used in subsequent tutorials have to be run using <code class="docutils literal notranslate"><span class="pre">sudo</span></code> and you may be prompted to enter your password.</p>
<p><strong>Note:</strong> The root user is disabled by default due to security reasons. If you want to login as root user, follow the below steps. Use the petalinux user’s password on the first password prompt, then set a new password for the root user. You can now login as root user using the newly set root user password.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xilinx-k26-starterkit-2020_2:~$ sudo su -l root

We trust you have received the usual lecture from the <span class="nb">local</span> System
Administrator. It usually boils down to these three things:

    <span class="c1">#1) Respect the privacy of others.</span>
    <span class="c1">#2) Think before you type.</span>
    <span class="c1">#3) With great power comes great responsibility.</span>

Password:
root@xilinx-k26-starterkit-2020_2:~# passwd
New password:
Retype new password:
passwd: password updated successfully
</pre></div>
</div>
</li>
<li><p>Get the latest application package.</p>
<ol>
<li><p>Check the package feed for new updates.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo dnf update
</pre></div>
</div>
<p>Confirm with “Y” when prompted to install new or updated packages.</p>
<p>Sometimes it is needed to clean the local dnf cache first. To do so, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo dnf clean all
</pre></div>
</div>
</li>
<li><p>Get the list of available packages in the feed.</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">xmutil</span> <span class="pre">getpkgs</span></code></p>
</li>
<li><p>Install the package with dnf install:</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">dnf</span> <span class="pre">install</span> <span class="pre">packagegroup-kv260-nlp-smartvision.noarch</span></code></p>
</li>
</ol>
<p>Note: For setups without access to the internet, it is possible to download and use the package locally. Please refer to the <a class="reference internal" href="../../local_package_feed.html"><span class="doc">Install from a local package feed</span></a> for instructions.</p>
</li>
<li><p>Dynamically load the application package.</p>
<ol>
<li><p>Show the list and status of available acceleration platforms and AI Applications:</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">xmutil</span> <span class="pre">listapps</span></code></p>
</li>
<li><p>Switch to a different platform for different AI Application:</p>
<ul>
<li><p>When there is no active accelerator by inspecting with xmutil listapps, just activate the one you want to switch.</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">xmutil</span> <span class="pre">loadapp</span> <span class="pre">kv260-nlp-smartvision</span></code></p>
</li>
<li><p>When there’s already an accelerator being activated, unload it first, then switch to the one you want.</p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">xmutil</span> <span class="pre">unloadapp</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">xmutil</span> <span class="pre">loadapp</span> <span class="pre">kv260-nlp-smartvision</span></code></p>
</li>
</ul>
</li>
</ol>
</li>
</ol>
</div>
<div class="section" id="run-the-application">
<h2>Run the Application<a class="headerlink" href="#run-the-application" title="Permalink to this heading">¶</a></h2>
<p>There are two ways to interact with the application.</p>
<div class="section" id="jupyter-notebook">
<h3>Jupyter notebook<a class="headerlink" href="#jupyter-notebook" title="Permalink to this heading">¶</a></h3>
<p>To launch Jupyter notebook on the target, run below command. Use Chrome web-browser to interact with the platform.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>// Fill <span class="k">in</span> ip-address from ifconfig. If the setup uses direct PC connection, use ifconfig eth0 &lt;ip-address&gt; to <span class="nb">set</span> the IP address to board
$  sudo jupyter-lab --ip<span class="o">=</span>&lt;ip-address&gt; --allow-root --notebook-dir<span class="o">=</span>/opt/xilinx/share/notebooks/ <span class="p">&amp;</span> 
</pre></div>
</div>
<p>Output example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>xilinx-SOM-multi-cc-2020_2:~$ sudo jupyter-lab --ip<span class="o">=</span>ip-address --allow-root --notebook-dir<span class="o">=</span>/opt/xilinx/share/notebooks/ <span class="p">&amp;</span>
<span class="o">[</span><span class="m">1</span><span class="o">]</span> <span class="m">1530</span>
xilinx-SOM-multi-cc-2020_2:~$ <span class="o">[</span>W <span class="m">15</span>:31:44.879 LabApp<span class="o">]</span> JupyterLab server extension not enabled, manually loading...
<span class="o">[</span>I <span class="m">15</span>:31:44.905 LabApp<span class="o">]</span> JupyterLab extension loaded from /usr/lib/python3.7/site-packages/jupyterlab
<span class="o">[</span>I <span class="m">15</span>:31:44.906 LabApp<span class="o">]</span> JupyterLab application directory is /usr/share/jupyter/lab
<span class="o">[</span>I <span class="m">15</span>:31:44.927 LabApp<span class="o">]</span> Serving notebooks from <span class="nb">local</span> directory: /opt/xilinx/share/notebooks
<span class="o">[</span>I <span class="m">15</span>:31:44.928 LabApp<span class="o">]</span> The Jupyter Notebook is running at:
<span class="o">[</span>I <span class="m">15</span>:31:44.928 LabApp<span class="o">]</span> http://xxx.xx.x.xxx:8888/?token<span class="o">=</span>635db1d645eeccc6a72bf1bb9c125164b1d689696348c97f
<span class="o">[</span>I <span class="m">15</span>:31:44.928 LabApp<span class="o">]</span>  or http://127.0.0.1:8888/?token<span class="o">=</span>635db1d645eeccc6a72bf1bb9c125164b1d689696348c97f
<span class="o">[</span>I <span class="m">15</span>:31:44.928 LabApp<span class="o">]</span> Use Control-C to stop this server and shut down all kernels <span class="o">(</span>twice to skip confirmation<span class="o">)</span>.
<span class="o">[</span>C <span class="m">15</span>:31:44.952 LabApp<span class="o">]</span>

    To access the notebook, open this file <span class="k">in</span> a browser:
        file:///home/root/.local/share/jupyter/runtime/nbserver-1531-open.html
    Or copy and paste one of these URLs:
        http://xxx.xx.x.xxx:8888/?token<span class="o">=</span>635db1d645eeccc6a72bf1bb9c125164b1d689696348c97f
     or http://127.0.0.1:8888/?token<span class="o">=</span>635db1d645eeccc6a72bf1bb9c125164b1d689696348c97f
</pre></div>
</div>
<p>In case user has started Jupyter-lab by running another Application supported on KV260 Vision AI Starter kit, user may skip the above step and does not need to restart.
In case user closes Chrome browser and need to find URL for Jupyter lab, run below command</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">sudo</span> <span class="pre">jupyter</span> <span class="pre">notebook</span> <span class="pre">list</span></code></p>
<p>Output example:</p>
<blockquote>
<div><p>Currently running servers:</p>
<p><code class="docutils literal notranslate"><span class="pre">http://ip:port/?token=xxxxxxxxxxxxxxxxxx</span></code>  :: /opt/xilinx/share/notebooks</p>
</div></blockquote>
<p>In the notebook, we will construct the GStreamer pipeline string, you can get it by adding simple python code to print it out, and played with gst-launch-1.0 command in the console, and there are some user options variables that can be changed and run with. For other parts of the pipeline, you can also change and play to see the effect easily.</p>
</div>
<div class="section" id="command-line">
<h3>Command line<a class="headerlink" href="#command-line" title="Permalink to this heading">¶</a></h3>
<p>This allow the user to run “nlp-smartvision” application on CLI. These are to be executed using the UART/debug interface.</p>
<hr class="docutils" />
<p><strong>NOTE</strong></p>
<p>Before running any of the commandline applications, we need to initialize the board to set media nodes and library path. Current application supports frames at 1024x768 resolution and RGB format</p>
<ul class="simple">
<li><p>Set media nodes configurations by running the below command. It will intialize the MIPI capture and DP/HDMI display pipeline. It will exit automatically after 10 sec.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>init-nlp-smartvision.sh
</pre></div>
</div>
<hr class="docutils" />
<p>Run the following command to launch the application for live audio input via USB microphone.
The user needs to be silent for the first few seconds (2.5s apx.) for the application to dynamically decide the noise threshold value as per user’s input device and enviornment. Once you see the following message “<em>Noise Threshold is set. You can start speaking the keywords now..</em>” you are ready to start pronouncing any of the ten keywords (Yes, No, Off, On, Up, Down, Left, Right, Stop, Go).</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision -l
</pre></div>
</div>
<p align="center"> (or) </p><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision --live-audio
</pre></div>
</div>
<p>The detected keyword will be displayed on the terminal and the corresponding action on the input video stream will be displayed on the monitor, which is connected to the board through DP/HDMI cable.</p>
<p>To print FPS along with the above application use -v or –verbose flag shown in the below command. The FPS is measured as average over 90 consecutive frames. Also the latency of keywords spotting + action is printed while the keyword is detected.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision -l -v
</pre></div>
</div>
<blockquote>
<div><p>You should be able to see the video the camera is capturing on the monitor connected to the board</p>
<ul class="simple">
<li><p>The application starts with facedetect. When there is a face captured by the camera, there should be a blue bounding box drawn around the face, and the box should follow the movement of the face.</p></li>
<li><p>Speak the desired keyword into the microphone, application will perform the following assigned tasks as mentioned below.</p></li>
</ul>
</div></blockquote>
<p><img alt="Keyword action mapping" src="../../../_images/keyword_mapping.png" /></p>
<blockquote>
<div><p>Note: Google Command dataset has audio clips of 1 second duration. Thus, the expectation by KWS task is that one keyword is spoken within a duration of 1 second.
Note: Stop command resets display setting, but does not change monitor on/off mode.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="file-based-testing-and-accuracy-measurement-of-kws-only">
<h2>File based Testing and Accuracy Measurement of KWS Only<a class="headerlink" href="#file-based-testing-and-accuracy-measurement-of-kws-only" title="Permalink to this heading">¶</a></h2>
<p>NLP SmartVision provides a mode which is dedicated for testing the accuracy of keyword spotting (no vision task is running during this mode) on pre-recorded audio files. User needs to provide audio files along with a text file that consists of paths to the audio files which are to be tested. The application expects the audio files to be grouped under folders with keyword as the folder name. Thus, the text file will consist of lines with keyword/*.wav paths corresponding to each audio file (example: yes/audio1.wav). For more details please refer <a class="reference external" href="#testing-accuracy-on-google-command-dataset">Testing Accuracy on Google Command Dataset</a> and <a class="reference external" href="#testing-custom-input-audio-files">Testing Custom Input Audio Files</a>.</p>
<p>The following command tests the audio files listed in the testing_list.txt file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## Change your dircetory to the dircetory where you have the testing_list.txt file having proper paths to the audio files.</span>
sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision -f testing_list.txt
</pre></div>
</div>
<p align="center"> (or) </p><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision --file-audio testing_list.txt
</pre></div>
</div>
<div class="section" id="testing-accuracy-on-google-command-dataset">
<h3>Testing Accuracy on Google Command Dataset<a class="headerlink" href="#testing-accuracy-on-google-command-dataset" title="Permalink to this heading">¶</a></h3>
<p>Users can download the open source Google’s speech command dataset for testing the application in file input mode. This dataset consists of pre-recorded audio files for 30 keywords and the audio files that are separated for testing are listed in the testing_list.txt file. Use the following commands on a linux local host machine to download and extract this dataset. These commands also create the datafiles that are required for testing the application with 10 keywords for which the model has been trained.</p>
<p><strong>Tip :</strong> You can copy the below commands and create a single script. Then directly execute that script to do all the required steps one after the other.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir Google_dataset
<span class="nb">cd</span> Google_dataset
wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz
tar -xf speech_commands_v0.01.tar.gz
mkdir keywords
mv -t ./keywords/ on off up down left right yes no stop go
sed -n -e <span class="s1">&#39;/down\//p; /go\//p; /left\//p; /no\//p; /off\//p; /on\//p; /right\//p; /stop\//p; /yes\//p; /up\//p &#39;</span> testing_list.txt &gt; ./keywords/testing_list.txt
find . -maxdepth <span class="m">1</span> ! -name keywords -print0<span class="p">|</span>xargs -0 rm -r --
</pre></div>
</div>
<p>These commands will create a directory with the name <code class="docutils literal notranslate"><span class="pre">Google_dataset/keywords</span></code> inside the current working directory on your local machine. Now, all the contents inside this keywords directory needs to copied onto to the microSD card which can be done without removing the microSD from board by using scp or via a USB stick. Otherwise remove and connect the microSD card to your local machine and copy the contents and place the microSD card back and boot the board again.</p>
<p><strong>Note :</strong> The commands may take few minutes (depending on the internet speed) to download and process the dataset.</p>
<p>Output after running the command for file based testing will also report the accuracy. Sample output on Google Command Dataset is shown below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Ground truth : yes            Predicted : yes
Ground truth : yes            Predicted : yes
Ground truth : yes            Predicted : <span class="nv">yes</span>
<span class="o">=========================================</span>
Number of keywords <span class="nv">tested</span> <span class="o">=</span> <span class="m">2552</span>
Number of keywords detected <span class="nv">correctly</span> <span class="o">=</span> <span class="m">2383</span>
<span class="nv">Accuracy</span> <span class="o">=</span> <span class="m">93</span>.3777%
</pre></div>
</div>
</div>
<div class="section" id="testing-custom-input-audio-files">
<h3>Testing Custom Input Audio Files<a class="headerlink" href="#testing-custom-input-audio-files" title="Permalink to this heading">¶</a></h3>
<p>The application expects audio file names to be stored as <code class="docutils literal notranslate"><span class="pre">keyword/audio_filename.wav</span></code> format into the audio files list file. For example, a pre-recorded audio file of keyword ‘yes’ needs to be listed as <code class="docutils literal notranslate"><span class="pre">yes/file_001.wav</span></code>. The application uses main directory name (‘yes’ in this example) as ground truth to compare against the detected keyword. New line character must be placed after every audio file name to differentiate multiple audio files (even after the last file name).  Moreover, audio file needs to be copied to the SD card into the directory from which the application will be invoked. For example, <code class="docutils literal notranslate"><span class="pre">/home/petalinux/keywords/yes/file_001.wav</span></code>.</p>
<p>The test audio files should have the following specifications:</p>
<ul class="simple">
<li><p>Sampling rate: 16 kHz</p></li>
<li><p>Sample width: 16 bits per sample</p></li>
<li><p>Sample encoding: Little endian</p></li>
<li><p>Number of channels: 1 (mono)</p></li>
<li><p>Supported format: S16_LE (PCM signed 16-bit little-endian)</p></li>
<li><p>Audio Length: 1 second</p></li>
</ul>
</div>
</div>
<div class="section" id="image-based-testing-of-dpu-only">
<h2>Image based Testing of DPU Only<a class="headerlink" href="#image-based-testing-of-dpu-only" title="Permalink to this heading">¶</a></h2>
<p>NLP SmartVision provides a mode which is dedicated for testing the Vision models on DPU (no KWS task is running during this mode) on image files. User needs to provide image files along with the AI model thats under test</p>
<p>The following command tests the image files.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span>/opt/xilinx/lib nlp-smartvision -t &lt;image.jpg/image.png&gt; &lt;model&gt;
</pre></div>
</div>
<p>The command returns the metadata along with a jpg fine containing bounding box on the input image</p>
</div>
<div class="section" id="files-structure-of-the-application">
<h2>Files structure of the application<a class="headerlink" href="#files-structure-of-the-application" title="Permalink to this heading">¶</a></h2>
<p>The application is installed as:</p>
<ul class="simple">
<li><p>Binary File: =&gt; /opt/xilinx/bin</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nlp-smartvision</td>
<td>Main application</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Script File: =&gt; /opt/xilinx/bin/</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>init-nlp-smartvision.sh</td>
<td>Configures media nodes to run RGB - MIPI DP/HDMI Pipeline</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Jupyter notebook file: =&gt; /opt/xilinx/share/notebooks/nlp-smartvision</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nlp-smartvision.ipynb</td>
<td>Jupyter notebook file for nlp-smartvision demo.</td>
</tr>
</tbody>
</table></div>
<div class="section" id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Go back to the <a class="reference internal" href="../nlp_smartvision_landing.html"><span class="doc">KV260 SOM NLP Smartvision design start page</span></a></p></li>
<li><p><a class="reference internal" href="../../building_the_design.html"><span class="doc">Building the Design Tutorial</span></a></p></li>
</ul>
<div class="section" id="license">
<h3>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h3>
<p>Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License.</p>
<p>You may obtain a copy of the License at
<a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>
<p align="center">Copyright&copy; 2021 Xilinx</p></div>
</div>
</div>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction_nlp.html" class="btn btn-neutral float-left" title="Design Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="customize_ai_models_nlp.html" class="btn btn-neutral float-right" title="Customizing the AI Models used in the application" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022, Xilinx, Inc. Xilinx is now a part of AMD.
      <span class="lastupdated">Last updated on August 1, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="https://pages.gitenterprise.xilinx.com/techdocs/Test/vvas/build/html/index.html#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>