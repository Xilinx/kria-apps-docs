<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
  <title>Design Overview &mdash; Kria™ KV260 2021.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Setting up the Board and Application Deployment" href="app_deployment_nlp.html" />
    <link rel="prev" title="NLP SmartVision" href="../nlp_smartvision_landing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../../index.html" class="icon icon-home"> Kria™ KV260
            <img src="../../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2021.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">SOM</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/">Landing Page</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/creating_applications.html">Application Development</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/ubuntu_support.html">Ubuntu Support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/bootfw.html">Boot Firmware</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kr260-docs.html">Kria KR260</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/KRS/">Kria Robotics Stack</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">KV260 Applications</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../smartcamera/smartcamera_landing.html">Smart Camera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aibox-reid/aibox_landing.html">AIBox-ReID</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../defect-detect/defectdetect_landing.html">Defect Detect</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../nlp_smartvision_landing.html">NLP SmartVision</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../nlp_smartvision_landing.html#overview">Overview</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#design-components">Design Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="#next-steps">Next Steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../nlp_smartvision_landing.html#features">Features</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#quick-start">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#tutorials">Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#architecture">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#other">Other</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Releases</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kv260/2020.2/build/html/index.html">2020.2</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Kria™ KV260</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../nlp_smartvision_landing.html">NLP SmartVision</a> &raquo;</li>
      <li>Design Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/nlp-smartvision/docs/introduction_nlp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <table class="sphinxhide">
 <tr>
   <td align="center"><img src="../../media/xilinx-logo.png" width="30%"/><h1> Kria&trade; KV260 Vision AI Starter Kit NLP Smart Vision Tutorial</h1>
   </td>
 </tr>
 <tr>
 <td align="center"><h1> Design Overview </h1> </td>
 </tr>
</table><div class="section" id="design-overview">
<h1>Design Overview<a class="headerlink" href="#design-overview" title="Permalink to this heading">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>The NLP SmartVision design built on KV260 Vision AI Starter Kit provides a framework for building and customizing audio &amp; video platforms that consist of four pipeline stages:</p>
<ul class="simple">
<li><p>Capture pipeline</p></li>
<li><p>Audio processing pipeline</p></li>
<li><p>Video processing pipeline</p></li>
<li><p>Output pipeline</p></li>
</ul>
<p>The design has a platform and integrated accelerator functions. The platform consists of Capture pipeline, Output pipeline and some video processing functions. This approach makes the design leaner and provides a user maximum Programmable Logic (PL) for the accelerator development. The platform supports audio capture from a USB microphone and video capture from MIPI single sensor device. The output can be displayed on a monitor via Display Port or HDMI.</p>
<p>The application continuously captures and process the audio data for performing the Keyword Spotting (KWS) task. As the name suggests, job of KWS is to process the audio data and detect (spot) the spoken keyword. Keyword detected is from a pre-defined set of keywords for which the model was trained. The KWS model used for NLP SmartVision is <a class="reference external" href="https://arxiv.org/abs/1711.07128">Hello Edge</a>, which was trained on 1 second audio files of 10 keywords (Yes, No, Off, On, Up, Down, Left, Right, Stop, Go) from the open sourced <a class="reference external" href="https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html">Google Command Dataset</a>.</p>
<p>For the vision tasks the following example acceleration functions can be run on this platform using programmable Deep Learning Processor Units (DPU)</p>
<ul class="simple">
<li><p>Face Detection         - Network model: cf_densebox_wider_360_640_1.11G_1.2</p></li>
<li><p>Object Detection       - Network model: dk_yolov2_voc_448_448_0.77_7.82G_1.2</p></li>
<li><p>Number Plate Detection - Network model: cf_plate-detection_320_320_0.49G_1</p></li>
</ul>
<p>The following figure shows the various pipelines supported by the design.</p>
<p><img alt="Pipelines Supported" src="../../../_images/pipelines1.png" /></p>
<p>The application processing unit (APU) in the Processing Sytem (PS) consists of four Arm® Cortex®-A53 cores and is configured to run in symmetric multi-processing (SMP) Linux mode in the  design. The application running on Linux is responsible for configuring and controlling the audio/video pipelines and accelerators using Jupyter notebooks or the application. The following figure shows the software state after the boot process has completed and the individual applications have been started on the APU. Details are described in the <a class="reference internal" href="sw_arch_platform_nlp.html"><span class="doc">Software Architecture document</span></a>.</p>
<p><img alt="Software state after application is running" src="../../../_images/software_state1.png" /></p>
<p>The APU application controls the following data paths implemented in a combination of the PS and PL:</p>
<ul class="simple">
<li><p>ALSA based live continuous audio data capture using USB interface controlled by the PS and writing audio frames into DDR memory.</p></li>
<li><p>Capture pipeline capturing video frames into DDR memory from an image sensor connected via MIPI CSI-2 Rx through the PL.</p></li>
<li><p>Memory-to-memory (M2M) pipeline</p>
<ul>
<li><p>Audio M2M Pipeline implements the KWS inference application. Audio frames are read from DDR and processed by the Hello Edge model running on PS. The detected keyword is then passed as an argument to the post-processing block and a corresponding pre-defined action is performed on the output video frame.</p></li>
<li><p>Video M2M Pipeline implements a typical Neural Network Inference Application. In this design the Neural Net is implemented in DPU, video frames are read from DDR memory, processed by the DPU, and then written back to memory.</p></li>
</ul>
</li>
<li><p>An output pipeline reads post-processed video frames from memory and sends the frames to a sink.</p>
<ul>
<li><p>In the display pipeline sink is a monitor. DP controller subsystem in the PS is coupled to STDP4320 de-multiplexer on the carrier card. STDP4320 consists of dual mode output ports configured as DP and HDMI.</p></li>
</ul>
</li>
</ul>
<p>The following figure shows an example end-to-end pipeline which could be a USB microphone as an audio source and a single image sensor as the video source. Audio source writes to DDR, followed by Voice Activity Detection (VAD) and Keyword Spotting Inference. Task of VAD is to keep monitoring the input audio data and whenever there is an activity (increase in input energies w.r.to audio input) create a 1 second audio frame and pass it to KWS for inference.
Similarly video capture is followed by pre-process and DPU IPs for application NN Inference. The inferred keyword and vision task are post processed and output video frames are displayed via DP splitter onto a DP display, as the video sink. The figure also shows the image preprocessing and postprocessing block in the capture and acceleration path. The preprocessing block scales down the video whereas the postprocessing block does several actions based on spoken audio command.  The video format in the figure is the output format on each block. Details are described in the <a class="reference internal" href="hw_arch_platform_nlp.html"><span class="doc">Hardware Architecture document</span></a>.</p>
<p><img alt="End to end example pipelines" src="../../../_images/end_to_end_pp.png" /></p>
</div>
<div class="section" id="design-components">
<h2>Design Components<a class="headerlink" href="#design-components" title="Permalink to this heading">¶</a></h2>
<details>
 <summary><b>Hardware components</b></summary><ul class="simple">
<li><p>KV260 Vision AI Starter Kit including</p>
<ul>
<li><p>USB Microphone</p></li>
<li><p>USB Camera</p></li>
<li><p>On Semi AP1302 Image Signal Processor (<a class="reference external" href="https://www.onsemi.com/products/sensors/image-sensors-processors/image-processors/ap1302">https://www.onsemi.com/products/sensors/image-sensors-processors/image-processors/ap1302</a>) on the carrier card</p></li>
<li><p>HDMI-DP splitter on the carrier card</p></li>
<li><p>On Semi AR1335 CMOS Image sensor (<a class="reference external" href="https://www.onsemi.com/products/sensors/image-sensors-processors/image-sensors/ar1335">https://www.onsemi.com/products/sensors/image-sensors-processors/image-sensors/ar1335</a>)</p></li>
<li><p>Raspberry pi camera module <a class="reference external" href="https://www.raspberrypi.com/products/camera-module-v2/">https://www.raspberrypi.com/products/camera-module-v2/</a></p></li>
</ul>
</li>
</ul>
</details><details>
 <summary><b>Interfaces and IP</b></summary><ul class="simple">
<li><p>Audio inputs</p>
<ul>
<li><p>USB Microphone</p></li>
</ul>
</li>
<li><p>Audio processing</p>
<ul>
<li><p>Customized ALSA based audio capture and audio packets preparation.</p></li>
<li><p>Audio pre-processing for voice activity detection (VAD) on PS</p></li>
<li><p>PS based accelaration of KWS using a subset of <a class="reference external" href="https://github.com/ARM-software/CMSIS_5/tree/a65b7c9a3e6502127fdb80eb288d8cbdf251a6f4">ARM CMSIS</a> library (ARM-DSP and ARM-NN).</p></li>
</ul>
</li>
<li><p>Video inputs</p>
<ul>
<li><p>MIPI CSI-2 Rx</p></li>
</ul>
</li>
<li><p>Video outputs</p>
<ul>
<li><p>DisplayPort</p></li>
<li><p>HDMI</p></li>
</ul>
</li>
<li><p>Video processing</p>
<ul>
<li><p>Accelerator functions on DPU</p></li>
<li><p>Xilinx ISP</p></li>
<li><p>PS based pre and post processing specific to a accelerator function</p></li>
</ul>
</li>
<li><p>Auxiliary Peripherals</p>
<ul>
<li><p>QSPI</p></li>
<li><p>SD</p></li>
<li><p>I2C</p></li>
<li><p>UART</p></li>
<li><p>Ethernet</p></li>
<li><p>General purpose I/O (GPIO)</p></li>
</ul>
</li>
</ul>
</details><details>
 <summary><b>Software components</b></summary><ul class="simple">
<li><p>Operating system</p>
<ul>
<li><p>APU: SMP Linux</p></li>
</ul>
</li>
<li><p>Linux kernel subsystems</p>
<ul>
<li><p>Video source: Video4 Linux (V4L2)</p></li>
<li><p>Audio source: ALSA</p></li>
<li><p>Display: Direct Rendering Manager (DRM)/Kernel Mode Setting (KMS)</p></li>
<li><p>Linux user space frameworks</p></li>
<li><p>Jupyter</p></li>
<li><p>GStreamer</p></li>
<li><p>OpenCV</p></li>
<li><p>Xilinx run-time (XRT)</p></li>
</ul>
</li>
</ul>
 </details> <details>
 <summary><b>Resolution and Format Supported</b></summary><ul class="simple">
<li><p>Resolutions</p>
<ul>
<li><p>1024x768 &#64;30FPS</p></li>
</ul>
</li>
<li><p>Pixel format</p>
<ul>
<li><p>YUV 4:2:2 (NV16)</p></li>
</ul>
</li>
<li><p>Audio Data</p>
<ul>
<li><p>Sampling Rate : 16kHz</p></li>
<li><p>Number of channels: 1 (mono)</p></li>
<li><p>Supported format: S16_LE (PCM signed 16-bit little-endian)</p></li>
</ul>
</li>
</ul>
 </details>
&nbsp;</div>
<div class="section" id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h2>
<p>The user can choose any of the following next steps:</p>
<ul class="simple">
<li><p><a class="reference internal" href="app_deployment_nlp.html"><span class="doc">Setting up the Board and Application Deployment Tutorial</span></a></p></li>
<li><p><a class="reference internal" href="../../building_the_design.html"><span class="doc">Building the Design Tutorial</span></a></p></li>
</ul>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<p>For more information refer to the documents below</p>
<ul class="simple">
<li><p>Kria KV260 Vision AI Starter Kit User Guide (<a class="reference external" href="https://www.xilinx.com/cgi-bin/docs/rdoc?t=som-doc%3Bv=latest%3Bd=ug1089-kv260-starter-kit.pdf">UG1089</a>)</p></li>
<li><p>Kria SOM Carrier Card Design Guide (<a class="reference external" href="https://www.xilinx.com/cgi-bin/docs/rdoc?t=som-doc%3Bd=ug1091-carrier-card-design.pdf">UG1091</a>)</p></li>
<li><p>Kria KV260 Vision AI Starter Kit Data Sheet(<a class="reference external" href="https://www.xilinx.com/cgi-bin/docs/ndoc?t=data_sheets%3Bd=ds986-kv260-starter-kit.pdf">DS986</a>)</p></li>
<li><p>Kria K26 SOM Data Sheet(<a class="reference external" href="https://www.xilinx.com/cgi-bin/docs/ndoc?t=data_sheets%3Bd=ds987-k26-som.pdf">DS987</a>)</p></li>
</ul>
<div class="section" id="license">
<h3>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h3>
<p>Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License.</p>
<p>You may obtain a copy of the License at
<a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>
<p align="center">Copyright&copy; 2021 Xilinx</p></div>
</div>
</div>


           </div>
          </div>
          
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../nlp_smartvision_landing.html" class="btn btn-neutral float-left" title="NLP SmartVision" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="app_deployment_nlp.html" class="btn btn-neutral float-right" title="Setting up the Board and Application Deployment" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2022, Xilinx, Inc. Xilinx is now a part of AMD.
      <span class="lastupdated">Last updated on September 9, 2022.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>