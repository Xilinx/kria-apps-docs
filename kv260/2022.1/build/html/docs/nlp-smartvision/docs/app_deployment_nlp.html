<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
<!-- OneTrust Cookies Consent Notice start for xilinx.github.io -->

<script src="https://cdn.cookielaw.org/scripttemplates/otSDKStub.js" data-document-language="true" type="text/javascript" charset="UTF-8" data-domain-script="03af8d57-0a04-47a6-8f10-322fa00d8fc7" ></script>
<script type="text/javascript">
function OptanonWrapper() { }
</script>
<!-- OneTrust Cookies Consent Notice end for xilinx.github.io -->
<!-- Google Tag Manager -->
<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
<!-- End Google Tag Manager -->
  <title>Setting up the Board and Application Deployment &mdash; Kria™ KV260 2022.1 documentation</title>
      <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
      <link rel="stylesheet" href="../../../_static/_static/custom.css" type="text/css" />
    <link rel="shortcut icon" href="../../../_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Customizing the AI Models used in the application" href="customize_ai_models_nlp.html" />
    <link rel="prev" title="Design Overview" href="introduction_nlp.html" /> 
</head>

<body class="wy-body-for-nav">

<!-- Google Tag Manager -->
<noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
<!-- End Google Tag Manager --> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
            <a href="../../../index.html" class="icon icon-home"> Kria™ KV260
            <img src="../../../_static/xilinx-header-logo.svg" class="logo" alt="Logo"/>
          </a>
              <div class="version">
                2022.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">SOM</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/">Landing Page</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/Kria_doc_map/map.htm">Kria Adventure Map</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/creating_applications.html">Application Development</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/ubuntu_support.html">Ubuntu Support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/bootfw.html">Boot Firmware</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/openamp.html">OpenAMP</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/dfx.html">Dynamic Function eXchange</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/ipmi_eeprom.html">IPMI EEPROM Design Guide</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/yocto.html">Yocto Support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/xen.html">XEN Support</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kd240-docs.html">Kria KD240</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kv260-docs.html">Kria KV260</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kr260-docs.html">Kria KR260</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/KRS/">Kria Robotics Stack</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">KV260 Applications</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../smartcamera/smartcamera_landing.html">Smart Camera</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../aibox-reid/aibox_landing.html">AI Box ReID</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../defect-detect/defectdetect_landing.html">Defect Detect</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../nlp_smartvision_landing.html">NLP SmartVision</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#overview">Overview</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../nlp_smartvision_landing.html#quick-start">Quick Start</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Setting up the Board and Application Deployment</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#booting-up-linux">Booting up Linux</a></li>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-the-board">Setting up the Board</a></li>
<li class="toctree-l4"><a class="reference internal" href="#application-specific-hardware-setup">Application Specific Hardware Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="#downloading-and-loading-application-firmware">Downloading and Loading Application Firmware</a></li>
<li class="toctree-l4"><a class="reference internal" href="#docker-based-application-preparation">Docker based application preparation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-the-application">Run the Application</a></li>
<li class="toctree-l4"><a class="reference internal" href="#file-based-testing-and-accuracy-measurement-of-kws-only">File based Testing and Accuracy Measurement of KWS Only</a></li>
<li class="toctree-l4"><a class="reference internal" href="#image-based-testing-of-dpu-only">Image based Testing of DPU Only</a></li>
<li class="toctree-l4"><a class="reference internal" href="#files-structure-of-the-application">Files structure of the application</a></li>
<li class="toctree-l4"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#tutorials">Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#architecture">Architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#repository">Repository</a></li>
<li class="toctree-l2"><a class="reference internal" href="../nlp_smartvision_landing.html#other">Other</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../aibox/aibox-dist_landing.html">AI Box Distributed ReID</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bist/bist_landing.html">Built-In Self Test (BIST)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Releases</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kv260/2021.1/build/html/index.html">2021.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/kria-apps-docs/kv260/2020.2/build/html/index.html">2020.2</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: black" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Kria™ KV260</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../nlp_smartvision_landing.html">NLP SmartVision</a> &raquo;</li>
      <li>Setting up the Board and Application Deployment</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <table class="sphinxhide">
 <tr>
   <td align="center"><img src="../../media/xilinx-logo.png" width="30%"/><h1> Kria&trade; KV260 Vision AI Starter Kit NLP SmartVision Tutorial</h1>
   </td>
 </tr>
 <tr>
 <td align="center"><h1>Setting up the Board and Application Deployment</h1> </td>
 </tr>
</table><div class="section" id="setting-up-the-board-and-application-deployment">
<h1>Setting up the Board and Application Deployment<a class="headerlink" href="#setting-up-the-board-and-application-deployment" title="Permalink to this heading">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>This document shows how to set up the board and run the nlp-smartvision application.</p>
<p>This guide and its prebuilt are targeted for Ubuntu 22.04 and Xilinx 2022.1 toolchain. The previous version of this application (on Xilinx 2021.1 toolchain) targeted to Petalinux is still available <a class="reference external" href="https://xilinx.github.io/kria-apps-docs/2021.1/build/html/index.html">online</a>.</p>
</div>
<div class="section" id="booting-up-linux">
<h2>Booting up Linux<a class="headerlink" href="#booting-up-linux" title="Permalink to this heading">¶</a></h2>
<p>Before continuing with NLP application specific instructions, if not yet done so, boot Linux with instructions from <a class="reference internal" href="../../kria_starterkit_linux_boot.html"><span class="doc">Kria Starter Kit Linux boot</span></a> page.</p>
</div>
<div class="section" id="setting-up-the-board">
<h2>Setting up the Board<a class="headerlink" href="#setting-up-the-board" title="Permalink to this heading">¶</a></h2>
</div>
<div class="section" id="application-specific-hardware-setup">
<h2>Application Specific Hardware Setup<a class="headerlink" href="#application-specific-hardware-setup" title="Permalink to this heading">¶</a></h2>
<p>Besides the hardware configurations required in <a class="reference internal" href="../../kria_starterkit_linux_boot.html"><span class="doc">Kria Starter Kit Linux boot</span></a> for booting Linux, AIBox application requires a 4k  monitor to display up to 4 channels of 1080p video.</p>
<p><img alt="GitHub Logo" src="../../../_images/som-board.png" /></p>
<ul>
<li><p>Monitor:</p>
<p>The monitor for this application must support 1024x768 resolution. Before booting, connect the monitor to the board via DP/HDMI port.</p>
</li>
<li><p>Camera sensors:</p>
<p>This application supports the below 3 camera modules</p>
<ul class="simple">
<li><p>AR1335 sensor module in J7</p></li>
<li><p>Raspberry pi sensor module in J9</p></li>
<li><p>USB webcam in any of the available USB ports.</p></li>
</ul>
<p>Install the required sensor modules in respective locations.</p>
</li>
<li><p>USB Microphone:</p>
<p>Connect the microphone to any of the USB ports. If you USB webcam has a build-in microphone , it will also acts as the USB microphone</p>
</li>
</ul>
</div>
<div class="section" id="downloading-and-loading-application-firmware">
<h2>Downloading and Loading Application Firmware<a class="headerlink" href="#downloading-and-loading-application-firmware" title="Permalink to this heading">¶</a></h2>
<ol>
<li><p>Download the firmware</p>
<ul>
<li><p>Search package feed for packages compatible with KV260</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ubuntu@kria:~$ sudo apt search xlnx-firmware-kv260
Sorting... Done
Full Text Search... Done
xlnx-firmware-kv260-aibox-reid/jammy <span class="m">0</span>.1-0xlnx1 arm64
 FPGA firmware <span class="k">for</span> Xilinx boards - kv260 aibox-reid application

xlnx-firmware-kv260-benchmark-b4096/jammy <span class="m">0</span>.1-0xlnx1 arm64
 FPGA firmware <span class="k">for</span> Xilinx boards - kv260 benchmark-b4096 application

xlnx-firmware-kv260-defect-detect/jammy <span class="m">0</span>.1-0xlnx1 arm64
 FPGA firmware <span class="k">for</span> Xilinx boards - kv260 defect-detect application

xlnx-firmware-kv260-nlp-smartvision/jammy,now <span class="m">0</span>.1-0xlnx1 arm64 <span class="o">[</span>installed<span class="o">]</span>
 FPGA firmware <span class="k">for</span> Xilinx boards - kv260 nlp-smartvision application

xlnx-firmware-kv260-smartcam/jammy <span class="m">0</span>.1-0xlnx1 arm64
 FPGA firmware <span class="k">for</span> Xilinx boards - kv260 smartcam application
</pre></div>
</div>
</li>
<li><p>Install firmware binaries</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo apt install xlnx-firmware-kv260-nlp-smartvision
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>Dynamically load the application package:</p>
<p>The firmware consists of bitstream, device tree overlay (dtbo) file. The firmware is loaded dynamically on user request once Linux is fully booted. The xmutil utility can be used for that purpose.</p>
<ul>
<li><p>After installing the FW, execute xmutil listapps to verify that it is captured under the listapps function, and to have dfx-mgrd re-scan and register all accelerators in the FW directory tree.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo xmutil listapps
</pre></div>
</div>
</li>
<li><p>Switch to a different platform for different Application:</p>
<p>When there’s already another accelerator/firmware being activated apart from xlnx-app-kv260-pmod-rs485-test, unload it first, then switch to xlnx-app-kv260-nlp-smartvision.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>sudo xmutil unloadapp
sudo xmutil loadapp kv260-nlp-smartvision
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
</div>
<div class="section" id="docker-based-application-preparation">
<h2>Docker based application preparation<a class="headerlink" href="#docker-based-application-preparation" title="Permalink to this heading">¶</a></h2>
<ol>
<li><p>Pull the 2022.1 docker image for nlp-smartvision using the below command.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker pull xilinx/nlp-smartvision:2022.1
</pre></div>
</div>
<p>Once the above step is done, you can check for the available images as shown below</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ubuntu@kria:~$ docker images -a
REPOSITORY                                                                  TAG        IMAGE ID       CREATED                  SIZE
xilinx/nlp-smartvision                                                      <span class="m">2022</span>.1     3c16ce65624a   Less than a second ago   <span class="m">1</span>.41GB
</pre></div>
</div>
<p>The storage volume on the SD card can be limited with multiple dockers. If there are space issues, use following command to remove the existing container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker rmi --force &lt;other containers&gt;
</pre></div>
</div>
</li>
<li><p>This application features display output either on Ubuntu GUI or the entire monitor as mentioned in the below table</p></li>
</ol>
<table border="1" class="docutils">
<thead>
<tr>
<th align="center">S.No</th>
<th align="left">Scenario</th>
<th align="center">Display sink</th>
<th align="left">Note</th>
<th align="left">Required steps</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">1</td>
<td align="left">User runs docker on a terminal opened on Ubuntu GUI</td>
<td align="center">ximagesink</td>
<td align="left">opens new window on Ubuntu GUI to display output</td>
<td align="left">disable access control for local docker with <code>xhost +local:docker</code></td>
</tr>
<tr>
<td align="center">2</td>
<td align="left">User runs docker on serial port or SSH session</td>
<td align="center">kmssink</td>
<td align="left">Uses entire monitor to display the output</td>
<td align="left">Disable Ubuntu GUI with <code>sudo xmutil desktop_disable</code></td>
</tr>
</tbody>
</table><blockquote>
<div><p><strong><em>NOTE:</em></strong>	Once you are done with running the application, please enable the access control with <code class="docutils literal notranslate"><span class="pre">xhost</span> <span class="pre">-local:docker</span></code></p>
</div></blockquote>
<ol>
<li><p>Launch the docker using the below command. User should launch the NLP Docker container as user “ubuntu” and not as other user or “sudo”.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker run <span class="se">\</span>
--env<span class="o">=</span><span class="s2">&quot;DISPLAY&quot;</span> <span class="se">\</span>
-h <span class="s2">&quot;xlnx-docker&quot;</span> <span class="se">\</span>
--env<span class="o">=</span><span class="s2">&quot;XDG_SESSION_TYPE&quot;</span> <span class="se">\</span>
--net<span class="o">=</span>host <span class="se">\</span>
--privileged <span class="se">\</span>
-v /tmp:/tmp <span class="se">\</span>
-v /dev:/dev <span class="se">\</span>
-v /sys:/sys <span class="se">\</span>
-v /etc/vart.conf:/etc/vart.conf <span class="se">\</span>
-v /lib/firmware/xilinx:/lib/firmware/xilinx <span class="se">\</span>
-v /run:/run <span class="se">\</span>
-it xilinx/nlp-smartvision:2022.1 bash
</pre></div>
</div>
<p>It will launch the nlp-smartvision image in a new container</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>root@xlnx-docker/#
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="run-the-application">
<h2>Run the Application<a class="headerlink" href="#run-the-application" title="Permalink to this heading">¶</a></h2>
<p>There are two ways to interact with application, via Jupyter notebook or Command line</p>
<div class="section" id="jupyter-notebook">
<h3>Jupyter notebook<a class="headerlink" href="#jupyter-notebook" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Launch the Jupyter notebook with <code class="docutils literal notranslate"><span class="pre">root</span></code> privilege using the following command:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>   jupyter lab --allow-root --notebook-dir<span class="o">=</span>/opt/xilinx/kv260-nlp-smartvision/share/notebooks/ --ip<span class="o">=</span>&lt;ip address&gt; <span class="p">&amp;</span>

    // fill <span class="k">in</span> ip-address from ifconfig, eth0
</pre></div>
</div>
<p>Output example:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.141 LabApp<span class="o">]</span> JupyterLab extension loaded from /usr/lib/python3.8/site-packages/jupyterlab
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.141 LabApp<span class="o">]</span> JupyterLab application directory is /usr/share/jupyter/lab
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.164 ServerApp<span class="o">]</span> jupyterlab <span class="p">|</span> extension was successfully loaded.
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.166 ServerApp<span class="o">]</span> Serving notebooks from <span class="nb">local</span> directory: /opt/xilinx/kv260-nlp-smartvision/share/notebooks/
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.166 ServerApp<span class="o">]</span> Jupyter Server <span class="m">1</span>.2.1 is running at:
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.166 ServerApp<span class="o">]</span> http://192.168.3.123:8888/lab?token<span class="o">=</span>9f7a9cd1477e8f8226d62bc026c85df23868a1d9860eb5d5
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.166 ServerApp<span class="o">]</span>  or http://127.0.0.1:8888/lab?token<span class="o">=</span>9f7a9cd1477e8f8226d62bc026c85df23868a1d9860eb5d5
<span class="o">[</span>I <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.167 ServerApp<span class="o">]</span> Use Control-C to stop this server and shut down all kernels <span class="o">(</span>twice to skip confirmation<span class="o">)</span>.
<span class="o">[</span>C <span class="m">2021</span>-08-02 <span class="m">15</span>:54:31.186 ServerApp<span class="o">]</span>

    To access the server, open this file <span class="k">in</span> a browser:
        file:///root/.local/share/jupyter/runtime/jpserver-1119-open.html
    Or copy and paste one of these URLs:
        http://192.168.3.123:8888/lab?token<span class="o">=</span>9f7a9cd1477e8f8226d62bc026c85df23868a1d9860eb5d5
     or http://127.0.0.1:8888/lab?token<span class="o">=</span>9f7a9cd1477e8f8226d62bc026c85df23868a1d9860eb5d5
</pre></div>
</div>
<ul>
<li><p>User can access the server by opening the server URL from previous steps with the Chrome browser.</p>
<p>In the notebook, we will explain the usage of app and the commands needed to run live usecase</p>
</li>
</ul>
</div>
<div class="section" id="command-line">
<h3>Command line<a class="headerlink" href="#command-line" title="Permalink to this heading">¶</a></h3>
<p>This allow the user to run “nlp-smartvision” application on CLI. These are to be executed using the UART/debug interface.</p>
<p>Run the following command to launch the application for live audio input via USB microphone.
The user needs to be silent for the first few seconds (2.5s apx.) for the application to dynamically decide the noise threshold value as per user’s input device and environment. Once you see the following message “<em>Noise Threshold is set. You can start speaking the keywords now..</em>” you are ready to start pronouncing any of the ten keywords (Yes, No, Off, On, Up, Down, Left, Right, Stop, Go).</p>
<div class="section" id="application-usage">
<h4>Application Usage<a class="headerlink" href="#application-usage" title="Permalink to this heading">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Usage: nlp-smartvision <span class="o">[</span>OPTION<span class="o">]</span> <span class="o">[</span>arg1<span class="o">]</span> <span class="o">[</span>arg2<span class="o">]</span>

-h <span class="o">(</span>or<span class="o">)</span> --help                                    <span class="nb">help</span>
-m <span class="o">(</span>or<span class="o">)</span> --mipi &lt;isp/rpi&gt;                          <span class="nb">test</span> the application with live video from mipi cameras rpi<span class="o">(</span>default<span class="o">)</span>/isp
-u <span class="o">(</span>or<span class="o">)</span> --usb                                     <span class="nb">test</span> the application with live video from USB camera
-f <span class="o">(</span>or<span class="o">)</span> --file-audio  &lt;testing_list&gt;.txt          <span class="nb">test</span> the keyword spotting with audio files listed <span class="k">in</span> the .txt file
-t <span class="o">(</span>or<span class="o">)</span> --test &lt;sample_image&gt; &lt;model&gt;             <span class="nb">test</span> the DPU with sample images. Input is Model and sample jpeg. Supported models are densebox_640_360, yolov2_voc_pruned_0_77 <span class="p">&amp;</span> plate_detect
-v <span class="o">(</span>or<span class="o">)</span> --verbose  
</pre></div>
</div>
</div>
<div class="section" id="run-the-app-with-default-mipi-sensor-rpi">
<h4>Run the app with default mipi sensor(RPI)<a class="headerlink" href="#run-the-app-with-default-mipi-sensor-rpi" title="Permalink to this heading">¶</a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nlp-smartvision -m
</pre></div>
</div>
<p align="center"> (or) </p><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nlp-smartvision --mipi
</pre></div>
</div>
<p>The detected keyword will be displayed on the terminal and the corresponding action on the input video stream will be displayed on the monitor, which is connected to the board through DP/HDMI cable.</p>
<p>To print FPS along with the above application use -v or –verbose flag shown in the below command. The FPS is measured as average over 90 consecutive frames. Also the latency of keywords spotting + action is printed while the keyword is detected.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nlp-smartvision -m -v
</pre></div>
</div>
<blockquote>
<div><p>You should be able to see the video the camera is capturing on the monitor connected to the board</p>
<ul class="simple">
<li><p>The application starts with facedetect. When there is a face captured by the camera, there should be a blue bounding box drawn around the face, and the box should follow the movement of the face.</p></li>
<li><p>Speak the desired keyword into the microphone, application will perform the following assigned tasks as mentioned below.</p></li>
</ul>
</div></blockquote>
<p><img alt="Keyword action mapping" src="../../../_images/keyword_mapping.png" /></p>
<blockquote>
<div><p>Note: Google Command dataset has audio clips of 1 second duration. Thus, the expectation by KWS task is that one keyword is spoken within a duration of 1 second.
Note: Stop command resets display setting, but does not change monitor on/off mode.</p>
</div></blockquote>
</div>
</div>
</div>
<div class="section" id="file-based-testing-and-accuracy-measurement-of-kws-only">
<h2>File based Testing and Accuracy Measurement of KWS Only<a class="headerlink" href="#file-based-testing-and-accuracy-measurement-of-kws-only" title="Permalink to this heading">¶</a></h2>
<p>NLP SmartVision provides a mode which is dedicated for testing the accuracy of keyword spotting (no vision task is running during this mode) on pre-recorded audio files. User needs to provide audio files along with a text file that consists of paths to the audio files which are to be tested. The application expects the audio files to be grouped under folders with keyword as the folder name. Thus, the text file will consist of lines with keyword/*.wav paths corresponding to each audio file (example: yes/audio1.wav). For more details please refer <a class="reference external" href="#testing-accuracy-on-google-command-dataset">Testing Accuracy on Google Command Dataset</a> and <a class="reference external" href="#testing-custom-input-audio-files">Testing Custom Input Audio Files</a>.</p>
<p>The following command tests the audio files listed in the testing_list.txt file.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1">## Change your dircetory to the dircetory where you have the testing_list.txt file having proper paths to the audio files.</span>
nlp-smartvision -f testing_list.txt
</pre></div>
</div>
<p align="center"> (or) </p><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nlp-smartvision --file-audio testing_list.txt
</pre></div>
</div>
<div class="section" id="testing-accuracy-on-google-command-dataset">
<h3>Testing Accuracy on Google Command Dataset<a class="headerlink" href="#testing-accuracy-on-google-command-dataset" title="Permalink to this heading">¶</a></h3>
<p>Users can download the open source Google’s speech command dataset for testing the application in file input mode. This dataset consists of pre-recorded audio files for 30 keywords and the audio files that are separated for testing are listed in the testing_list.txt file. Use the following commands to download and extract this dataset. These commands also create the datafiles that are required for testing the application with 10 keywords for which the model has been trained.</p>
<p><strong>Tip :</strong> You can copy the below commands and create a single script. Then directly execute that script to do all the required steps one after the other.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir Google_dataset
<span class="nb">cd</span> Google_dataset
wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz
tar -xf speech_commands_v0.01.tar.gz
mkdir keywords
mv -t ./keywords/ on off up down left right yes no stop go
sed -n -e <span class="s1">&#39;/down\//p; /go\//p; /left\//p; /no\//p; /off\//p; /on\//p; /right\//p; /stop\//p; /yes\//p; /up\//p &#39;</span> testing_list.txt &gt; ./keywords/testing_list.txt
find . -maxdepth <span class="m">1</span> ! -name keywords -print0<span class="p">|</span>xargs -0 rm -r --
</pre></div>
</div>
<p>These commands will create a directory with the name <code class="docutils literal notranslate"><span class="pre">Google_dataset/keywords</span></code> inside the current working directory.
<strong>Note :</strong> The commands may take few minutes (depending on the internet speed) to download and process the dataset.</p>
<p>Output after running the command for file based testing will also report the accuracy. Sample output on Google Command Dataset is shown below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Ground truth : yes            Predicted : yes
Ground truth : yes            Predicted : yes
Ground truth : yes            Predicted : <span class="nv">yes</span>
<span class="o">=========================================</span>
Number of keywords <span class="nv">tested</span> <span class="o">=</span> <span class="m">2552</span>
Number of keywords detected <span class="nv">correctly</span> <span class="o">=</span> <span class="m">2383</span>
<span class="nv">Accuracy</span> <span class="o">=</span> <span class="m">93</span>.3777%
</pre></div>
</div>
</div>
<div class="section" id="testing-custom-input-audio-files">
<h3>Testing Custom Input Audio Files<a class="headerlink" href="#testing-custom-input-audio-files" title="Permalink to this heading">¶</a></h3>
<p>The application expects audio file names to be stored as <code class="docutils literal notranslate"><span class="pre">keyword/audio_filename.wav</span></code> format into the audio files list file. For example, a pre-recorded audio file of keyword ‘yes’ needs to be listed as <code class="docutils literal notranslate"><span class="pre">yes/file_001.wav</span></code>. The application uses main directory name (‘yes’ in this example) as ground truth to compare against the detected keyword. New line character must be placed after every audio file name to differentiate multiple audio files (even after the last file name).  Moreover, audio file needs to be copied to the SD card into the directory from which the application will be invoked. For example, <code class="docutils literal notranslate"><span class="pre">/keywords/yes/file_001.wav</span></code>.</p>
<p>The test audio files should have the following specifications:</p>
<ul class="simple">
<li><p>Sampling rate: 16 kHz</p></li>
<li><p>Sample width: 16 bits per sample</p></li>
<li><p>Sample encoding: Little endian</p></li>
<li><p>Number of channels: 1 (mono)</p></li>
<li><p>Supported format: S16_LE (PCM signed 16-bit little-endian)</p></li>
<li><p>Audio Length: 1 second</p></li>
</ul>
</div>
</div>
<div class="section" id="image-based-testing-of-dpu-only">
<h2>Image based Testing of DPU Only<a class="headerlink" href="#image-based-testing-of-dpu-only" title="Permalink to this heading">¶</a></h2>
<p>NLP SmartVision provides a mode which is dedicated for testing the Vision models on DPU (no KWS task is running during this mode) on image files. User needs to provide image files along with the AI model that’s under test</p>
<p>The following command tests the image files.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nlp-smartvision -t &lt;image.jpg/image.png&gt; &lt;model&gt;
</pre></div>
</div>
<p>The command returns the metadata along with a jpg file containing bounding box on the input image</p>
</div>
<div class="section" id="files-structure-of-the-application">
<h2>Files structure of the application<a class="headerlink" href="#files-structure-of-the-application" title="Permalink to this heading">¶</a></h2>
<p>The application is installed as:</p>
<ul class="simple">
<li><p>Binary File: =&gt; /opt/xilinx/kv260-nlp-smartvision/bin</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nlp-smartvision</td>
<td>Main application</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Script File: =&gt; /opt/xilinx/kv260-nlp-smartvision/bin/</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>init-isp-smartvision.sh</td>
<td>Configures ISP media nodes to run 1024.768@RGB</td>
</tr>
<tr>
<td>init-imx-smartvision.sh</td>
<td>Configures RPI media nodes to run 1024.768@RGB</td>
</tr>
<tr>
<td>nlp-smartvision.app</td>
<td>Application executable</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Jupyter notebook file: =&gt; /opt/xilinx/kv260-nlp-smartvision/share/notebooks/nlp-smartvision</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>filename</th>
<th>description</th>
</tr>
</thead>
<tbody>
<tr>
<td>nlp-smartvision.ipynb</td>
<td>Jupyter notebook file for nlp-smartvision demo.</td>
</tr>
</tbody>
</table></div>
<div class="section" id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>Go back to the <a class="reference internal" href="../nlp_smartvision_landing.html"><span class="doc">KV260 SOM NLP Smartvision design start page</span></a></p></li>
<li><p><a class="reference internal" href="../../building_the_design.html"><span class="doc">Building the Design Tutorial</span></a></p></li>
</ul>
<div class="section" id="license">
<h3>License<a class="headerlink" href="#license" title="Permalink to this heading">¶</a></h3>
<p>Licensed under the Apache License, Version 2.0 (the “License”); you may not use this file except in compliance with the License.</p>
<p>You may obtain a copy of the License at
<a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
<p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>
<p align="center">Copyright&copy; 2021 Xilinx</p></div>
</div>
</div>


           </div>
          </div>
          
				  
				  <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="introduction_nlp.html" class="btn btn-neutral float-left" title="Design Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="customize_ai_models_nlp.html" class="btn btn-neutral float-right" title="Customizing the AI Models used in the application" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021-2024, Advanced Micro Devices, Inc.
      <span class="lastupdated">Last updated on February 15, 2024.
      </span></p>
  </div>



										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">

													                    <div class="row">
                        <div class="col-xs-24">
                          <p><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a> | <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a> | <a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a> | <a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a> | <a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a> | <a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a> | <a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></p>
                        </div>
                    </div>
												</div>
											</div>
										</div>
										
</br>


  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 <script type="text/javascript">
    $(document).ready(function() {
        $(".toggle > *").hide();
        $(".toggle .header").show();
        $(".toggle .header").click(function() {
            $(this).parent().children().not(".header").toggle(400);
            $(this).parent().children(".header").toggleClass("open");
        })
    });
</script>


</body>
</html>